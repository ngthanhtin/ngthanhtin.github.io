<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tin Nguyen (Kevin)</title>
  
  <meta name="author" content="Tin Nguyen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
	<style>
        body {
    font-family: 'Arial', sans-serif;
    margin: 0;
    padding: 20px;
    background-color: #f4f4f4;
}

.profile {
    background-color: #fff;
    padding: 20px;
    margin: auto;
    max-width: 800px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    text-align: center; /* Center align text for a more friendly appearance */
}

.icon {
    margin-right: 10px;
}

/* Assign unique colors to different icons */
.fa-user-graduate { color: #1E90FF; } /* Dodger blue for academic achievements */
.fa-handshake { color: #28A745; } /* Green for collaboration, signaling go-ahead */
.fa-envelope { color: #FFC107; } /* Amber for communication, inviting contacts */
.fa-smile-beam { color: #FFD700; } /* Gold color for a warm, friendly vibe */


a {
    text-decoration: none;
    color: #007bff;
}

a:hover {
    text-decoration: underline;
}

    </style>

  
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><b>Tin Nguyen (Kevin)</b></name>
              </p>
              <p><i class="fas fa-user-graduate icon"></i>I'm Tin Nguyen, currently pursuing my PhD in Explainable AI at <a href="https://www.auburn.edu/">Auburn University, AL, US</a> under the mentorship of <a href="https://anhnguyen.me/lab/">Professor Anh Totti Nguyen</a>.
		       I received <b>Master</b> degree at <a href="http://en.sejong.ac.kr/eng/index.do">Sejong University, Seoul, Korea</a> under the supervision of <a href="https://scholar.google.com/citations?hl=en&user=gGfbXFIAAAAJ">Professor Yong-Guk Kim</a>.
		      I did my <b>Bachelor</b> at the <a href="https://en.hcmus.edu.vn/">University of Science, Ho Chi Minh City, Viet Nam </a> with guidance from <a href="https://scholar.google.com/citations?hl=en&user=MskoD4gAAAAJ">Professor Ly Quoc Ngoc</a>.
	      </p>
		<p>I'm keen on collaborating and sharing ideas to advance our understanding and application of AI &#128516;. If you're interested in working together or just want to chat about AI, feel free to <a href="mailto:ngthanhtinqn@gmail.com">get in touch</a><i class="fas fa-envelope email-icon"></i></p>
		    
              <p style="text-align:center">
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=zSAfD80AAAAJ&hl=vi">Google Scholar</a> &nbsp/&nbsp
		<a href="https://www.researchgate.net/profile/Tin-Nguyen-52">ResearchGate</a> &nbsp/&nbsp
                <a href="https://twitter.com/TnNguyn21237605">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/ngthanhtin/">Github</a>
              </p>
		
	      <p style="text-align:center">
                <a href="news.html">Latest News</a> &nbsp/&nbsp
                <a href="projects.html">Feature Projects</a>
              </p>
		    
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/tin.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/tin-circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="news"><tbody>
		<heading id="research"><b>Research</b></heading>
              <p>I am deeply committed to developing interpretable, editable, and robust machine learning approaches, though my interests extend beyond these core areas.
                Representative papers are <span class="highlight">highlighted</span>. 
              </p>
        </tbody>
	</table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	
		  <heading id="publications"><b>Conference</b></heading>

	<tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/peeb.png' width="180", height="150">
          </div>
          <script type="text/javascript">
            function hypernerf_start() {
              document.getElementById('hypernerf_image').style.opacity = "1";
            }

            function hypernerf_stop() {
              document.getElementById('hypernerf_image').style.opacity = "0";
            }
            hypernerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <papertitle>[Accepted] PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck</papertitle>
          </a>
          <br>
          <a href=""><strong>Thang Pham, Peijie Chen, Tin Nguyen</strong>, Seunghyun Yoon, Trung Bui, Anh Nguyen</a><br>
          <em> NAACL,</em> 2024 Findings
          <br>
          <a href="https://github.com/anguyen8/peeb">Code</a> /
          <a href="https://arxiv.org/pdf/2403.05297.pdf">Paper</a> /
	  <a href="https://huggingface.co/spaces/XAI/PEEB">Demo</a>
          <p></p>
          <p>We proposed a part-based bird classifier that makes predictions based on part-wise descriptions. Our method directly utilizes human-provided descriptions (in this work, from GPT4). It outperforms CLIP and M&V by 10 points in CUB and 28 points in NABirds.</p>
        </td>
      </tr>
	      
<!-- 	<tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/overview_habitat.png' width="180", height="150">
          </div>
          <script type="text/javascript">
            function hypernerf_start() {
              document.getElementById('hypernerf_image').style.opacity = "1";
            }

            function hypernerf_stop() {
              document.getElementById('hypernerf_image').style.opacity = "0";
            }
            hypernerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <papertitle>[Under Review] Leveraging Habitat Information for Fine-grained Bird Identification</papertitle>
          </a>
          <br>
          <a href=""><strong>Tin Nguyen</strong></a>,
          <a href="">Anh Nguyen</a> <br>
          <br>
          <em> ArXiv Preprint,</em> 2023
          <br>
<!--           <a href="">Project Page</a> / -->
<!--           <a href="">Code</a> /
          <a href="https://arxiv.org/abs/2312.14999">Paper</a>
          <p></p>
          <p>We are the first to explore integrating habitat information, one of the four major cues for identifying birds by ornithologists, into modern bird classifiers.</p>
        </td>
      </tr> -->
	       
      </table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	
		  <heading id="publications"><b>Journal</b></heading>
      
		  <!-- paper  -->
      <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/meme_ieee.png' width="180", height="150">
          </div>
          <script type="text/javascript">
            function hypernerf_start() {
              document.getElementById('hypernerf_image').style.opacity = "1";
            }

            function hypernerf_stop() {
              document.getElementById('hypernerf_image').style.opacity = "0";
            }
            hypernerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <papertitle>[Accepted] Meme Analysis using LLM-based Contextual Information and U-net Encapsulated Transformer</papertitle>
          </a>
          <br>
          <a href=""><strong>Thanh Tin Nguyen</strong></a>,
          <a href=""><strong>Marvin John</strong></a>,
	  <a href="">Hulin Jin</a>,
          <a href="">Yong-Guk Kim</a> <br>
          <br>
          <em> IEEE Access,</em> Jul, 4, 2024
          <br>
          <a href="">Project Page</a> /
          <a href="https://github.com/ignaciomarvinjohn/meme-uet-hmt">Code</a> /
          <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10589379">Paper</a>
          <p></p>
          <p>This study proposes an attention-based module for analyzing the sentiment and emotion of memes.</p>
        </td>
      </tr>
	      
      <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/vizdoom_instruction.gif' width="180", height="100">
          </div>
          <script type="text/javascript">
            function hypernerf_start() {
              document.getElementById('hypernerf_image').style.opacity = "1";
            }

            function hypernerf_stop() {
              document.getElementById('hypernerf_image').style.opacity = "0";
            }
            hypernerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <papertitle>[Accepted] Coarse-To-Fine Fusion for Language Grounding in 3D Navigation</papertitle>
          </a>
          <br>
          <a href=""><strong>Thanh Tin Nguyen</strong></a>,
	  <a href="https://vohoanganh.github.io/"><strong>Anh H. Vo</strong></a>,
	  <a href="">Soo-Mi Choi</a>,
          <a href="">Yong-Guk Kim</a> <br>
          <br>
          <em> Knowledge-based Systems (KBS),</em> Jul 4, 2023
          <br>
          <a href="https://youtu.be/rwODCJFM1SY">Video</a> /
          <a href="https://github.com/ngthanhtin/AE_VLN_Vizdoom">Code</a> /
          <a href="https://pdf.sciencedirectassets.com/271505/AIP/1-s2.0-S095070512300535X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEEkaCXVzLWVhc3QtMSJHMEUCIExkwGUkkQkvQwV2oqgKYGIm1HeosSdyPPb2mTW0Z0CrAiEAli20E8KQrsvhft5rJqhYx3Vwcj9%2BZZbA5aOnoSD8AGcqvAUIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDE%2FA%2BJC0iVmL3%2B90dSqQBdlS2%2BgFkIMT71YupcBo5dc9lohho23VCsYw4a8nJPJtFzfvSKxb5r4GWDk8UsMdUf%2BPNuMRwpfxtAz4lMgM0FFK2OLKYZm3NzODCn6ejndJvi3wbB153YHnDwTr2cBOuAmvXdZMI4VBOyx90CH65749lBSYOjMV2H9nKS32p4HOYbozET60yVhhSY5mh54yz8G%2BzPMReazYLJaBZbeBGTW6yI0wa5zOSK%2BlHDCtEIbUQLngzyBJH39oLz9lTKuOsxST8u1DcThHg39Tujt7PylKnAGRQUhkg8m2OvqwqbQfyuKT501Kqz7%2FwT2uY8RXj2b0FB8klPC9uniNSilMDtwLepTVaNP%2B59Az6dlrkINSvtcfz90RFaUW85qNt1kyuzBTS2dEh%2FnLSy1wYDgMIC%2BWHUo%2BDdc1CQpxDCToTuuaqWkUGJqNg%2BSCrScpFl8OUeChhLLRUelQZzEmjIkbd%2Bt75aoNKyMFBawfwgtFr3JKoXp7gH6VrSSxOtZ%2BTBlLpyXVcXjg%2FOzAGC0vTG%2BTmNKE%2F%2F05fZ48qzcsktSZC7Ydpl%2BPuaO16jaiNwXewse1W01VbLU00V9hiA02JXo6TDPfkhu3qznfsP9eJkrENfAF7v%2BbRsqS4yYP0fRqJI1T%2FrpwstNcnLnlDSYuL5y0dotAjTOZjLc8xo0nUcl533Oe0nQyW5vHvdi5XJgBwLknRfaulspJ76R6DFI9gDz8VtZrOzpYYq4RIQpagBpj%2BZ1dx9%2BZEAvm%2BrqlHMZWDbbD0cqsWJkBTM51FPJYXbAN1jUHsGV%2BzPFfM%2FiyrzL3GlPIPn%2F98Kqm3JNvXbPghoEpoDIq2CcGoUoRGarGIn4FmPt49npKY%2BNZt%2BG54jgnOcPTMK%2BFtqUGOrEBCibrY1wV%2Fl%2FOs1w695xeojsvBQq1oP6lW074RwdIJLgqZiOkK6kz5zE2tz2Fe2MuoB4vFg7tsogxtqLX5hWzE6SJfTfyX1SsWjqYTk87RuJ4ZhjymcYHbSvGZHP3R%2FFzfTNJ4xQZV8meNM7inRrfBrBub87Ahx%2FHWYVE5w%2FvOXuzjQltjZq19nLflx12VKLp6Ct7Ud31x8%2FTWkiyg52nJrkG0KjmIEsjeTrXlHcP50ox&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230711T171810Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3GHYG545%2F20230711%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=d96c3fdcb7e232361065375d6ef431e34c57343b1b6ce0cb5b4d6f414d55309d&hash=5adf7a86c57684d8cd4453027b352adffde07d14645e58cf9cce88a74a232c3c&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S095070512300535X&tid=spdf-1de94a62-bc90-412d-b7a0-518d4ae3ecbe&sid=7f83a1a75405c4460c8a934899df12ad2c27gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f1c560a5a5c04525553&rr=7e52b4458e7ae1c2&cc=us&fbclid=IwAR0ndSR1zVMbBpDmscw_pRNXWrVX8IaNuAp0yCUo9snsfHBND1G3Yw6OSfc">Paper</a>
          <p></p>
          <p>This study proposes a coarse-to-fine fusion module between vision and language. This will help an agent learn a joint representation while navigating in a virtual environment.</p>
        </td>
      </tr> 
	      
	<tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/aivn_covid.png' width="180", height="100">
              </div>
              <script type="text/javascript">
                function hypernerf_start() {
                  document.getElementById('hypernerf_image').style.opacity = "1";
                }

                function hypernerf_stop() {
                  document.getElementById('hypernerf_image').style.opacity = "0";
                }
                hypernerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>[Accepted] Fruit-CoV: An Efficient Vision-based Framework for Speedy Detection and Diagnosis of SARS-CoV-2 Infections Through Recorded Cough Sounds</papertitle>
              </a>
              <br>
							<a href="">Long H. Nguyen</a>,
							<a href="">Nhat Truong Pham</a>,
							<a href="">Van Huong Do</a>,
							<a href="">Liu Tai Nguyen</a>,
							<a href=""><strong>Thanh Tin Nguyen</strong></a>,
							<a href="">Van Dung Do</a>,
							<a href="">Hai Nguyen</a>, 
							<a href="">Ngoc Duy Nguyen</a> <br>
              <br>
              <font color="red"><strong>(Challenge 1st)</strong></font> <em>Expert System with Applications (ESWA)</em>, 1, November, 2022
              <br>
              <a href="https://www.covid.aihub.vn/organizers">Link Challenge</a> /
              <a href="https://www.sciencedirect.com/science/article/pii/S0957417422022308">Paper</a>
              <p></p>
              <p>Introducing Fruit-CoV, a two-stage vision framework, which is capable of detecting SARS-CoV-2 infections through recorded cough sounds. In this challenge, we won <b>100mil VND (~ $4275)</b> for the 1st place.</p>
            </td>
          </tr>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='hypernerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/hypernerf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/stereo_localization.gif' width="180", height="155">
              </div>
              <script type="text/javascript">
                function hypernerf_start() {
                  document.getElementById('hypernerf_image').style.opacity = "1";
                }

                function hypernerf_stop() {
                  document.getElementById('hypernerf_image').style.opacity = "0";
                }
                hypernerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>A New Framework of Moving Object Tracking Based on Object
					Detection-Tracking with Removal of Moving Features using Stereo Camera and IMU</papertitle>
              </a>
              <br>
							<a href=""><strong>Nguyen Thanh Tin</strong></a>,
							<a href="">Ly Quoc Ngoc</a>, 
							<a href="">Le Bao Tuan</a> <br>
              <br>
              <em>International Journal of Advanced Computer Science and Applications (SAI)</em>, 14, April, 2020
              <br>
              <a href="https://youtu.be/3n9yK8TkvFY">Video 1</a> /
			        <a href="https://youtu.be/CYT-qYVahKc">Video 2</a> / 
			        <a href="https://youtu.be/B69EYVtIKCo">Video 3</a> /
              <a href="https://thesai.org/Publications/ViewPaper?Volume=11&Issue=4&Code=IJACSA&SerialNo=6">Paper</a>
              <p></p>
              <p>Applying Yolo3 to Particle Filter to enhance its speed and accuracy, 
                furthermore, an end-to-end localization framework using a stereo camera and IMU in the unknown environment.</p>
            </td>
          </tr> 
		</tbody></table>
		  
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
		  <heading><b>Workshop / Challenge</b></heading>	
			
        <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/meme.jpg' width="190">
            </div>
            <script type="text/javascript">
              function mipnerf_start() {
                document.getElementById('mipnerf_image').style.opacity = "1";
              }

              function mipnerf_stop() {
                document.getElementById('mipnerf_image').style.opacity = "0";
              }
              mipnerf_stop()
            </script>
          </td>
		
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="">
              <papertitle>HCILab at Memotion 2.0 2022: Analysis of sentiment, emotion, and intensity of emotion classes from meme images using single and multi modalities</papertitle>
            </a>
            <br>
            <strong>Nguyen Thanh Tin</strong>, Nhat Truong Pham, Yong-Guk Kim, et. al,. 
            <br>
            <font color="red"><strong>(Workshop)</strong></font> <em>First Workshop on ​Multimodal Fact-Checking and Hate Speech Detection AAAI 2022</em>, 2021 &nbsp 
            <br>
            <a href="http://ceur-ws.org/Vol-3199/paper12.pdf">Paper</a> /
            <a href="https://aiisc.ai/defactify/memotion_2.html">Link Challenge</a> /
            <a href="https://ngthanhtin.github.io/blog/challenge/2021-11-15-memotion2/">Project Page</a> /
            <a href="https://github.com/ngthanhtin/Memotion2_AAAI_WS_2022">Code</a>
            <p></p>
            <p>Achieved 1st on the public leaderboard, applying SAN, multihop, CNNRoBerta as multimodalities, and Only Text and Image as Single modalities.</p>
          </td>
        </tr> 

          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/vlsp.gif" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/image_captioning_vlsp.png' width="200", height="140">
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }

                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Image Captioning Using Swin Transformer Encoder and LSTM Attention Decoder</papertitle>
              </a>
              <br>
              <strong>Nguyen Thanh Tin</strong>
              <br>
              <font color="red"><strong>(Workshop 3rd and)</strong></font> <em>VLSP - vieCap4H Challenge: Automatic image caption generation for
                 healthcare domains in Vietnamese (Oral presentation)</em>, 25, October, 2021 &nbsp 
              <br>
              <br>
              <font color="red"><strong>(VNU Journal of Science (JCSCE))</strong></font> <em>vieCap4H - VLSP 2021: Vietnamese Image Captioning for 
                Healthcare Domain using Swin Transformer and Attention-based LSTM</em>, 5, May, 2022 &nbsp 
              <br>
              <a href="https://jcsce.vnu.edu.vn/index.php/jcsce/article/view/369" type="application/pdf">Paper</a> /
			        <a href="https://vlsp.org.vn/vlsp2021/eval/vieCap4H">Link Challenge</a> /
              <a href="https://ngthanhtin.github.io/blog/challenge/2021-11-15-vietnameseimagecaptioningvlsp/">Project Page</a> /
              <a href="https://github.com/ngthanhtin/VLSP_ImageCaptioning">Code</a>
              <p></p>
              <p><b>[Most interesting discussed idea award] </b> Achieved 3rd on the private leaderboard, applying Swin Transformer as the Encoder (and other types), and LSTM Attention as the Decoder.</p>
              <p>Choosen to be in the Special Issue of VNU Journal of Science <a href="https://jcsce.vnu.edu.vn/index.php/jcsce">(JCSCE)</a></p>
            </td>
          </tr> 


        </tbody>
      </table>	

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
     		<tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading id="service"><b>Reviewer</b></heading>
                  

                  <ul style="list-style-type:square;">
                    <li>
                      <a href="">EMNLP, ACL (Top Conference in NLP)</a>
		    </li>
		    <li>
		      <a href="">Knowledge-based Systems (Q1 Journal)</a>
                    </li>

                  </ul>

              </td>
            </tr>

        </tbody></table>
	
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
     		<tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading id="service"><b>Academic Service</b></heading>
                  

                  <ul style="list-style-type:square;">
                    <!-- 1 -->
                    <li>
                      <a href="">[Aug, 2022 - May, 2023] Teacher at K-6 AI Club:
                         AI Club for children around the elementary school age (K-6) to learn math, coding, robotics, and artificial intelligence. This is a completely FREE, voluntary, educational event. Supported by Auburn University and an NSF CAREER award.</a>
                    </li>

                  </ul>

              </td>
            </tr>

        </tbody></table>

        <hr>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=faCcSvsbzaN24Thb3pbWgNeoMhkjEmdnhVxqV62bvyI&cl=ffffff&w=a"></script></td>
             <td width="75%" valign="center">
              <br>
<!--               <a href=""><p style="text-align:right;">Thanh-Tin Nguyen</p></a> -->
            </td>
          </tr>
          
        </tbody></table>


      </td>
    </tr>
  </table>

</body>

</html>
